"use strict";(self.webpackChunkdhenara_docs=self.webpackChunkdhenara_docs||[]).push([[1775],{2052:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>r,default:()=>p,frontMatter:()=>i,metadata:()=>a,toc:()=>d});const a=JSON.parse('{"id":"dhenara-ai/samples/text-gen/text-gen-async","title":"Async Text Generation","description":"Text Generation in Async Mode","source":"@site/docs/dhenara-ai/samples/text-gen/text-gen-async.md","sourceDirName":"dhenara-ai/samples/text-gen","slug":"/dhenara-ai/samples/text-gen/text-gen-async","permalink":"/dhenara-ai/samples/text-gen/text-gen-async","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"title":"Async Text Generation"},"sidebar":"dhenaraAiSidebar","previous":{"title":"Text Generation","permalink":"/dhenara-ai/samples/text-gen/"},"next":{"title":"Streaming","permalink":"/dhenara-ai/samples/text-gen/streaming"}}');var o=t(4848),s=t(8453);const i={title:"Async Text Generation"},r="Text Generation: Async",c={},d=[{value:"Text Generation in Async Mode",id:"text-generation-in-async-mode",level:2}];function l(e){const n={code:"code",h1:"h1",h2:"h2",header:"header",p:"p",pre:"pre",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"text-generation-async",children:"Text Generation: Async"})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'\nimport asyncio\nfrom dhenara.ai import AIModelClient\nfrom dhenara.ai.types import AIModelCallConfig, AIModelEndpoint\nfrom dhenara.ai.types.external_api import AIModelAPIProviderEnum\nfrom dhenara.ai.types.genai import AIModelAPI\nfrom dhenara.ai.types.genai.foundation_models.anthropic.chat import Claude37Sonnet\n\n# 1. Create an API\n# This can be used to create multiple model endpoints for the same API provider\napi = AIModelAPI(\n    provider=AIModelAPIProviderEnum.ANTHROPIC,\n    api_key="your_api_key", # TODO: Update with your Anthropic API Key\n)\n\n# 2. Select or create an AI model\n# You can either use the foundation models as it is, or create your own models\nmodel = Claude37Sonnet\n\n# Create the model endpoint\nmodel_endpoint = AIModelEndpoint(api=api, ai_model=model)\n\n# Create the client\nclient = AIModelClient(\n    model_endpoint=model_endpoint,\n    config=AIModelCallConfig(\n        max_output_tokens=16000,\n        reasoning=True,  # thinking/reasoning mode\n        max_reasoning_tokens=8000,  # Needed only if reasoning is set\n        streaming=False,\n    ),\n    is_async=True,  # async mode\n)\n\n\nasync def generate_text_async():\n    response = await client.generate_async(\n        prompt={\n            "role": "user",\n            "content": "Explain quantum computing to a high school student.",\n        },\n        context=[],\n        instructions=[\n            "Be concise and focus on practical applications.",\n        ],\n    )\n    print_response(response)\n\n\n# -----------------------------------------------------------------------------\n# To see formatted response, use below helper fn\ndef print_response(response):\n    for choice in response.chat_response.choices:\n        for content in choice.contents:\n            # Some formatting to differentiate contents\n            # With reasoning=True, same response will have multiple contents\n            print("-" * 80)\n            print(f"Type:: {content.type}")\n            print("-" * 80)\n            # Actual response text\n            print(f"{content.get_text()}")\n\n    # Optionally get the usage and cost for this call.\n    # Usage/Cost calculation is enabled by default, but can be disabled via setting\n    print("-" * 80)\n    print(f"Usage: {response.chat_response.usage}")\n    print(f"Usage Charge: {response.chat_response.usage_charge}")\n    print("-" * 80)\n\n\n# Call response  formatting fn\nprint_response(response)\n'})}),"\n",(0,o.jsx)(n.h2,{id:"text-generation-in-async-mode",children:"Text Generation in Async Mode"}),"\n",(0,o.jsx)(n.p,{children:"To use async in all API calls update your configuration in above example with is_async=True`"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"\nclient = AIModelClient(\n    model_endpoint=model_endpoint,\n    config=AIModelCallConfig(\n        max_output_tokens=16000,\n        reasoning=True,  # thinking/reasoning mode\n        max_reasoning_tokens=8000,  # Needed only if reasoning is set\n        streaming=False,\n    ),\n    is_async=True,  # NOTE: This was changed\n)\n\n\n# -----------------------------------------------------------------------------\n\nasyncio.run(generate_text_async())\n\n"})})]})}function p(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(l,{...e})}):l(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>i,x:()=>r});var a=t(6540);const o={},s=a.createContext(o);function i(e){const n=a.useContext(s);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:i(e.components),a.createElement(s.Provider,{value:n},e.children)}}}]);