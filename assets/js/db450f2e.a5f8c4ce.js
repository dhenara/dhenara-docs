"use strict";(self.webpackChunkdhenara_docs=self.webpackChunkdhenara_docs||[]).push([[309],{7731:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>u,frontMatter:()=>a,metadata:()=>o,toc:()=>d});const o=JSON.parse('{"id":"features/resource-configuration","title":"Resource Configuration","description":"The ResourceConfig class provides a centralized way to manage all AI Models and API credentials in Dhenara AI. This page explains how to use this powerful feature to simplify your application\'s resource management.","source":"@site/docs/features/resource-configuration.md","sourceDirName":"features","slug":"/features/resource-configuration","permalink":"/features/resource-configuration","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"title":"Resource Configuration"},"sidebar":"docsSidebar","previous":{"title":"Multi-Turn Conversations","permalink":"/features/multi-turn-conversations"},"next":{"title":"Usage & Charge Data","permalink":"/features/usasge-and-charge"}}');var t=i(4848),r=i(8453);const a={title:"Resource Configuration"},s="Resource Configuration",l={},d=[{value:"Overview",id:"overview",level:2},{value:"Getting Started",id:"getting-started",level:2},{value:"Creating a Configuration",id:"creating-a-configuration",level:3},{value:"Accessing Resources",id:"accessing-resources",level:3},{value:"Multi-Turn Conversations using Resource Config",id:"multi-turn-conversations-using-resource-config",level:2},{value:"Additional Features",id:"additional-features",level:2},{value:"Loading Custom Models",id:"loading-custom-models",level:3},{value:"Creating Custom Credentials Templates",id:"creating-custom-credentials-templates",level:3},{value:"Checking Available Endpoints",id:"checking-available-endpoints",level:3},{value:"Benefits",id:"benefits",level:2},{value:"Implementation Notes",id:"implementation-notes",level:2}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"resource-configuration",children:"Resource Configuration"})}),"\n",(0,t.jsxs)(n.p,{children:["The ",(0,t.jsx)(n.code,{children:"ResourceConfig"})," class provides a centralized way to manage all AI Models and API credentials in Dhenara AI. This page explains how to use this powerful feature to simplify your application's resource management."]}),"\n",(0,t.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,t.jsxs)(n.p,{children:["When working with multiple AI models and providers, managing credentials and configurations can be challenging. The ",(0,t.jsx)(n.code,{children:"ResourceConfig"})," class offers an elegant solution by:"]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Loading credentials from standardized configuration files"}),"\n",(0,t.jsx)(n.li,{children:"Automatically initializing API clients with the correct credentials"}),"\n",(0,t.jsx)(n.li,{children:"Creating model endpoints by matching models with compatible APIs"}),"\n",(0,t.jsx)(n.li,{children:"Providing a query interface to retrieve resources by attributes"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"getting-started",children:"Getting Started"}),"\n",(0,t.jsx)(n.h3,{id:"creating-a-configuration",children:"Creating a Configuration"}),"\n",(0,t.jsx)(n.p,{children:"The simplest way to get started is by creating a credentials template and then loading it:"}),"\n",(0,t.jsxs)(n.p,{children:["Also, you can copy a generated ",(0,t.jsx)(n.a,{href:"https://github.com/dhenara/dhenara/blob/master/src/dhenara/ai/types/resource/credentials.yaml",children:"template from GitHub"}),". (Below command will generate the same file)"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from dhenara.ai.types import ResourceConfig\n\n# Create a template credentials file\nResourceConfig.create_credentials_template("my_credentials.yaml")\n\n# Now edit my_credentials.yaml with your actual API keys\n\n# Load the configuration\nconfig = ResourceConfig()\nconfig.load_from_file("my_credentials.yaml", init_endpoints=True)\n'})}),"\n",(0,t.jsx)(n.p,{children:"The generated credentials template will include all supported providers with placeholders for API keys and other required credentials:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"# Dhenara AI Provider Credentials\n# Replace placeholder values with your actual API keys and remove unused items\n\nopenai:\n  api_key: <YOUR_OPENAI_API_KEY>\n\ngoogle_gemini_api:\n  api_key: <YOUR_GOOGLE_GEMINI_API_API_KEY>\n\nanthropic:\n  api_key: <YOUR_ANTHROPIC_API_KEY>\n\n# Additional providers...\n"})}),"\n",(0,t.jsx)(n.h3,{id:"accessing-resources",children:"Accessing Resources"}),"\n",(0,t.jsx)(n.p,{children:"Once loaded, you can access AI model endpoints using different methods:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Get an API by provider name\nanthropic_api = resource_config.get_api(AIModelAPIProviderEnum.ANTHROPIC)\n\n# Get an endpoint by model name\nmodel_endpoint = resource_config.get_model_endpoint(model_name="claude-3-5-haiku")\n\n# Use the endpoint with AIModelClient\nfrom dhenara.ai import AIModelClient\nclient = AIModelClient(endpoint)\n'})}),"\n",(0,t.jsx)(n.p,{children:"For more advanced queries, you can use the resource query interface:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from dhenara.ai.types import ResourceConfigItem, ResourceConfigItemTypeEnum\n\n# Get an endpoint by model name and specific API provider\nresource_item = ResourceConfigItem(\n    item_type=ResourceConfigItemTypeEnum.ai_model_endpoint,\n    query={"model_name": "gpt-4o", "api_provider": "openai"}\n)\n\n# Retrieve the endpoint\nendpoint = config.get_resource(resource_item)\n\n# Use the endpoint with AIModelClient\nfrom dhenara.ai import AIModelClient\nclient = AIModelClient(endpoint)\n'})}),"\n",(0,t.jsx)(n.h2,{id:"multi-turn-conversations-using-resource-config",children:"Multi-Turn Conversations using Resource Config"}),"\n",(0,t.jsxs)(n.p,{children:["Let's see how to use ResourceConfig in a practical multi-turn conversation example. We will modify the ",(0,t.jsx)(n.a,{href:"/features/multi-turn-conversations#real-world-usage",children:"Multi-Turn Conversations Example"})," using ResourceConfig:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import datetime\nimport random\n\nfrom dhenara.ai import AIModelClient\nfrom dhenara.ai.providers.common.prompt_formatter import PromptFormatter\nfrom dhenara.ai.types import AIModelCallConfig, AIModelEndpoint, ResourceConfig\nfrom dhenara.ai.types.conversation._node import ConversationNode\nfrom dhenara.ai.types.external_api import AIModelAPIProviderEnum\nfrom dhenara.ai.types.genai.foundation_models.anthropic.chat import Claude35Haiku, Claude37Sonnet\nfrom dhenara.ai.types.genai.foundation_models.google.chat import Gemini20Flash, Gemini20FlashLite\nfrom dhenara.ai.types.genai.foundation_models.openai.chat import GPT4oMini, O3Mini\n\n# Initialize resource config with credentials\nresource_config = ResourceConfig()\nresource_config.load_from_file(\n    credentials_file="~/.dhenara_credentials.yaml",  # Path to your credentials file\n    init_endpoints=False,  # Do not automatically create endpoints for all foundation models\n)\n\n# Manually set up endpoints\nanthropic_api = resource_config.get_api(AIModelAPIProviderEnum.ANTHROPIC)\nopenai_api = resource_config.get_api(AIModelAPIProviderEnum.OPEN_AI)\ngoogle_api = resource_config.get_api(AIModelAPIProviderEnum.GOOGLE_AI)\n\n# Create various model endpoints\nresource_config.model_endpoints = [\n    AIModelEndpoint(api=anthropic_api, ai_model=Claude37Sonnet),\n    AIModelEndpoint(api=anthropic_api, ai_model=Claude35Haiku),\n    AIModelEndpoint(api=openai_api, ai_model=O3Mini),\n    AIModelEndpoint(api=openai_api, ai_model=GPT4oMini),\n    AIModelEndpoint(api=google_api, ai_model=Gemini20Flash),\n    AIModelEndpoint(api=google_api, ai_model=Gemini20FlashLite),\n]\n\n\ndef get_context(previous_nodes: list[ConversationNode], destination_model: Any) -> list[Any]:\n    """Process previous conversation nodes into context for the next turn."""\n    context = []\n\n    for node in previous_nodes:\n        prompts = PromptFormatter.format_conversion_node_as_prompts(\n            model=destination_model,\n            user_query=node.user_query,\n            attached_files=node.attached_files,\n            previous_response=node.response,\n        )\n        context.extend(prompts)\n\n    return context\n\n\ndef handle_conversation_turn(\n    user_query: str,\n    instructions: list[str],\n    endpoint: AIModelEndpoint,\n    conversation_nodes: list[ConversationNode],\n) -> ConversationNode:\n    """Process a single conversation turn with the specified model and query."""\n\n    client = AIModelClient(\n        model_endpoint=endpoint,\n        config=AIModelCallConfig(\n            max_output_tokens=1000,\n            streaming=False,\n        ),\n        is_async=False,\n    )\n\n    # Format the user query\n    prompt = PromptFormatter.format_conversion_node_as_prompts(\n        model=endpoint.ai_model,\n        user_query=user_query,\n        attached_files=[],\n        previous_response=[],\n    )[0]\n\n    # Get context from previous turns (if any)\n    context = get_context(conversation_nodes, endpoint.ai_model) if conversation_nodes else []\n\n    # Generate response\n    response = client.generate(\n        prompt=prompt,\n        context=context,\n        instructions=instructions,\n    )\n\n    # Create conversation node\n    node = ConversationNode(\n        user_query=user_query,\n        attached_files=[],\n        response=response.chat_response,\n        timestamp=datetime.datetime.now().isoformat(),\n    )\n\n    return node\n\n\ndef run_multi_turn_conversation():\n    multi_turn_queries = [\n        "Tell me a short story about a robot learning to paint.",\n        "Continue the story but add a twist where the robot discovers something unexpected.",\n        "Conclude the story with an inspiring ending.",\n    ]\n\n    # Instructions for each turn\n    instructions_by_turn = [\n        ["Be creative and engaging."],\n        ["Build upon the previous story seamlessly."],\n        ["Bring the story to a satisfying conclusion."],\n    ]\n\n    # Store conversation history\n    conversation_nodes = []\n\n    # Process each turn\n    for i, query in enumerate(multi_turn_queries):\n        # Choose a random model endpoint\n        model_endpoint = random.choice(resource_config.model_endpoints)\n        # OR use a specific model\n        # model_endpoint = resource_config.get_model_endpoint(model_name=Claude35Haiku.model_name)\n\n        print(f"\ud83d\udd04 Turn {i + 1} with {model_endpoint.ai_model.model_name} from {model_endpoint.api.provider}\\n")\n\n        node = handle_conversation_turn(\n            user_query=query,\n            instructions=instructions_by_turn[i],\n            endpoint=model_endpoint,\n            conversation_nodes=conversation_nodes,\n        )\n\n        # Display the conversation\n        print(f"User: {query}")\n        print(f"Model: {model_endpoint.ai_model.model_name}\\n")\n        print(f"Model Response:\\n {node.response.choices[0].contents[0].get_text()}\\n")\n        print("-" * 80)\n\n        # Update conversation history for next turn\n        conversation_nodes.append(node)\n\n\nif __name__ == "__main__":\n    run_multi_turn_conversation()\n'})}),"\n",(0,t.jsx)(n.h2,{id:"additional-features",children:"Additional Features"}),"\n",(0,t.jsx)(n.h3,{id:"loading-custom-models",children:"Loading Custom Models"}),"\n",(0,t.jsx)(n.p,{children:"You can load specific models into a ResourceConfig:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from dhenara.ai.types.genai.foundation_models.openai.chat import GPT4o, O3Mini\n\n# Initialize with only specific models\nresource_config = ResourceConfig()\nresource_config.load_from_file(\n    credentials_file="credentials.yaml",\n    models=[GPT4o, O3Mini],\n    init_endpoints=True\n)\n'})}),"\n",(0,t.jsx)(n.h3,{id:"creating-custom-credentials-templates",children:"Creating Custom Credentials Templates"}),"\n",(0,t.jsx)(n.p,{children:"Generate custom credentials templates for your specific needs:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from dhenara.ai.types import ResourceConfig\n\n# Create a template with specific output location\nResourceConfig.create_credentials_template(output_file="my_org_credentials.json")\n'})}),"\n",(0,t.jsx)(n.h3,{id:"checking-available-endpoints",children:"Checking Available Endpoints"}),"\n",(0,t.jsx)(n.p,{children:"Inspect available endpoints:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# List all configured endpoints\nfor endpoint in resource_config.model_endpoints:\n    print(f"Model: {endpoint.ai_model.model_name}, Provider: {endpoint.api.provider}")\n'})}),"\n",(0,t.jsx)(n.h2,{id:"benefits",children:"Benefits"}),"\n",(0,t.jsxs)(n.p,{children:["The ",(0,t.jsx)(n.code,{children:"ResourceConfig"})," approach offers several advantages:"]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Separation of concerns"})," - Keep credentials separate from your application code"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Configuration as code"})," - Define your AI resources declaratively"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Consistent interface"})," - Access all AI models through a unified API"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Flexible provider mapping"})," - Use different API providers for the same model type"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Automatic resource management"})," - Let the system handle the details of API initialization"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"implementation-notes",children:"Implementation Notes"}),"\n",(0,t.jsxs)(n.p,{children:["The ",(0,t.jsx)(n.code,{children:"ResourceConfig"})," system internally manages the mapping between foundation models (like GPT-4o, Claude 3, etc.) and the API providers that can serve them (OpenAI, Azure OpenAI, etc.). This abstraction allows your application code to focus on what AI capabilities you need rather than worrying about the specific API implementation details."]}),"\n",(0,t.jsx)(n.p,{children:"By centralizing credential management, it also improves security by keeping sensitive information out of your application code and configuration versioning systems."})]})}function u(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>s});var o=i(6540);const t={},r=o.createContext(t);function a(e){const n=o.useContext(r);return o.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),o.createElement(r.Provider,{value:n},e.children)}}}]);