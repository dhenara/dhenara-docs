"use strict";(self.webpackChunkdhenara_docs=self.webpackChunkdhenara_docs||[]).push([[230],{582:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>p,frontMatter:()=>s,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"features/multi-turn/description","title":"Code Explained","description":"One of Dhenara\'s most powerful features is its ability to seamlessly manage multi-turn conversations with AI models. The code in previius page guide demonstrates how to create coherent, context-aware conversations across multiple turns, even when switching between different AI models and providers.","source":"@site/docs/features/multi-turn/description.md","sourceDirName":"features/multi-turn","slug":"/features/multi-turn/description","permalink":"/features/multi-turn/description","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"title":"Code Explained"},"sidebar":"docsSidebar","previous":{"title":"Real World Usage","permalink":"/features/multi-turn/code"},"next":{"title":"Usage & Charge Data","permalink":"/features/usasge-and-charge"}}');var t=o(4848),r=o(8453);const s={title:"Code Explained"},a="Multi-Turn Conversations with Dhenara",l={},d=[{value:"Overview",id:"overview",level:2},{value:"Key Features",id:"key-features",level:3},{value:"How It Works",id:"how-it-works",level:2},{value:"Example: Building a Multi-Turn Conversation",id:"example-building-a-multi-turn-conversation",level:2},{value:"Advantages Over Other Libraries",id:"advantages-over-other-libraries",level:2},{value:"1. Simplified Provider Switching",id:"1-simplified-provider-switching",level:3},{value:"2. Clean State Management",id:"2-clean-state-management",level:3},{value:"3. Full Control Over Instructions",id:"3-full-control-over-instructions",level:3},{value:"4. Type Safety and Reliability",id:"4-type-safety-and-reliability",level:3},{value:"5. Provider-Agnostic Implementation",id:"5-provider-agnostic-implementation",level:3},{value:"Common Use Cases",id:"common-use-cases",level:2},{value:"Next Steps",id:"next-steps",level:2},{value:"Conclusion",id:"conclusion",level:2}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"multi-turn-conversations-with-dhenara",children:"Multi-Turn Conversations with Dhenara"})}),"\n",(0,t.jsxs)(n.p,{children:["One of Dhenara's most powerful features is its ability to seamlessly manage multi-turn conversations with AI models. The code in ",(0,t.jsx)(n.a,{href:"/features/multi-turn/code",children:"previius page"})," guide demonstrates how to create coherent, context-aware conversations across multiple turns, even when switching between different AI models and providers."]}),"\n",(0,t.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,t.jsx)(n.p,{children:"Multi-turn conversations allow your application to maintain context across separate interactions with AI models. This is essential for creating natural dialogue flows, chatbots, interactive assistants, and any application where the conversation history matters."}),"\n",(0,t.jsx)(n.h3,{id:"key-features",children:"Key Features"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Cross-Provider Memory"}),": Seamlessly switch between models from different providers (OpenAI, Anthropic, Google) while preserving context"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Conversation History Management"}),": Simple structure for storing and accessing conversation history"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Dynamic Model Selection"}),": Flexibility to choose different models for each conversation turn"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Per-Turn Instructions"}),": Set specific instructions for each conversation turn"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Strongly Typed"}),": Clean, type-safe implementation with Pydantic models"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"how-it-works",children:"How It Works"}),"\n",(0,t.jsxs)(n.p,{children:["Dhenara uses a simple yet powerful system to manage conversation state through ",(0,t.jsx)(n.code,{children:"ConversationNode"})," objects, which store each turn's query, response, and metadata. The ",(0,t.jsx)(n.code,{children:"PromptFormatter"})," handles the conversion of conversation history into formats appropriate for each provider."]}),"\n",(0,t.jsx)(n.h2,{id:"example-building-a-multi-turn-conversation",children:"Example: Building a Multi-Turn Conversation"}),"\n",(0,t.jsx)(n.p,{children:"Here's a complete implementation showing how to create a multi-turn conversation that switches between AI models:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import datetime\nimport random\nfrom typing import Any\n\nfrom dhenara.ai import AIModelClient\nfrom dhenara.ai.providers.common.prompt_formatter import PromptFormatter\nfrom dhenara.ai.types import AIModelCallConfig, AIModelEndpoint\nfrom dhenara.ai.types.conversation._node import ConversationNode\nfrom dhenara.ai.types.external_api import AIModelAPIProviderEnum\nfrom dhenara.ai.types.genai import AIModelAPI\nfrom dhenara.ai.types.genai.foundation_models.anthropic.chat import Claude35Haiku, Claude37Sonnet\nfrom dhenara.ai.types.genai.foundation_models.google.chat import Gemini20Flash, Gemini20FlashLite\nfrom dhenara.ai.types.genai.foundation_models.openai.chat import GPT4oMini, O3Mini\n\n# Initialize API configurations\nanthropic_api = AIModelAPI(\n    provider=AIModelAPIProviderEnum.ANTHROPIC,\n    api_key="your_anthropic_api_key",\n)\nopenai_api = AIModelAPI(\n    provider=AIModelAPIProviderEnum.OPEN_AI,\n    api_key="your_openai_api_key",\n)\ngoogle_api = AIModelAPI(\n    provider=AIModelAPIProviderEnum.GOOGLE_AI,\n    api_key="your_google_api_key",\n)\n\n\n# Create various model endpoints\nall_model_endpoints = [\n    AIModelEndpoint(api=anthropic_api, ai_model=Claude37Sonnet),\n    AIModelEndpoint(api=anthropic_api, ai_model=Claude35Haiku),\n    AIModelEndpoint(api=openai_api, ai_model=O3Mini),\n    AIModelEndpoint(api=openai_api, ai_model=GPT4oMini),\n    AIModelEndpoint(api=google_api, ai_model=Gemini20Flash),\n    AIModelEndpoint(api=google_api, ai_model=Gemini20FlashLite),\n]\n\n\ndef get_context(previous_nodes: list[ConversationNode], destination_model: Any) -> list[Any]:\n    """Process previous conversation nodes into context for the next turn."""\n    context = []\n\n    for node in previous_nodes:\n        prompts = PromptFormatter.format_conversion_node_as_prompts(\n            model=destination_model,\n            user_query=node.user_query,\n            attached_files=node.attached_files,\n            previous_response=node.response,\n        )\n        context.extend(prompts)\n\n    return context\n\n\ndef handle_conversation_turn(\n    user_query: str,\n    instructions: list[str],\n    endpoint: AIModelEndpoint,\n    conversation_nodes: list[ConversationNode],\n) -> ConversationNode:\n    """Process a single conversation turn with the specified model and query."""\n\n    client = AIModelClient(\n        model_endpoint=endpoint,\n        config=AIModelCallConfig(\n            max_output_tokens=1000,\n            streaming=False,\n        ),\n        is_async=False,\n    )\n\n    # Format the user query\n    prompt = PromptFormatter.format_conversion_node_as_prompts(\n        model=endpoint.ai_model,\n        user_query=user_query,\n        attached_files=[],\n        previous_response=[],\n    )[0]\n\n    # Get context from previous turns (if any)\n    context = get_context(conversation_nodes, endpoint.ai_model) if conversation_nodes else []\n\n    # Generate response\n    response = client.generate(\n        prompt=prompt,\n        context=context,\n        instructions=instructions,\n    )\n\n    # Create conversation node\n    node = ConversationNode(\n        user_query=user_query,\n        attached_files=[],\n        response=response.chat_response,\n        timestamp=datetime.datetime.now().isoformat(),\n    )\n\n    return node\n\n\ndef run_multi_turn_conversation():\n    multi_turn_queries = [\n        "Tell me a short story about a robot learning to paint.",\n        "Continue the story but add a twist where the robot discovers something unexpected.",\n        "Conclude the story with an inspiring ending.",\n    ]\n\n    # Instructions for each turn\n    instructions_by_turn = [\n        ["Be creative and engaging."],\n        ["Build upon the previous story seamlessly."],\n        ["Bring the story to a satisfying conclusion."],\n    ]\n\n    # Store conversation history\n    conversation_nodes = []\n\n    # Process each turn\n    for i, query in enumerate(multi_turn_queries):\n        # Choose a random model endpoint\n        model_endpoint = random.choice(all_model_endpoints)\n        # OR choose if fixed order as\n        # model_endpoint = all_model_endpoints[i]\n\n        print(f"\ud83d\udd04 Turn {i + 1} with {model_endpoint.ai_model.model_name} from {model_endpoint.api.provider}\\n")\n\n        node = handle_conversation_turn(\n            user_query=query,\n            instructions=instructions_by_turn[i],  # Only if you need to change instruction on each turn, else leave []\n            endpoint=model_endpoint,\n            conversation_nodes=conversation_nodes,\n        )\n\n        # Display the conversation\n        print(f"User: {query}")\n        print(f"Model: {model_endpoint.ai_model.model_name}\\n")\n        print(f"Model Response:\\n {node.response.choices[0].contents[0].get_text()}\\n")\n        print("-" * 80)\n\n        # Append to nodes, so that next turn will have the context generated\n        conversation_nodes.append(node)\n\n\nif __name__ == "__main__":\n    run_multi_turn_conversation()\n'})}),"\n",(0,t.jsx)(n.h2,{id:"advantages-over-other-libraries",children:"Advantages Over Other Libraries"}),"\n",(0,t.jsx)(n.p,{children:"Dhenara's approach to multi-turn conversations offers several advantages:"}),"\n",(0,t.jsx)(n.h3,{id:"1-simplified-provider-switching",children:"1. Simplified Provider Switching"}),"\n",(0,t.jsx)(n.p,{children:"Unlike other libraries that may require different handlers for different providers, Dhenara's abstraction allows you to seamlessly switch between models from different providers (OpenAI, Anthropic, Google) while maintaining conversation context."}),"\n",(0,t.jsx)(n.h3,{id:"2-clean-state-management",children:"2. Clean State Management"}),"\n",(0,t.jsxs)(n.p,{children:["The ",(0,t.jsx)(n.code,{children:"ConversationNode"})," structure provides a clear, intuitive way to manage conversation history without complex memory chains or callbacks."]}),"\n",(0,t.jsx)(n.h3,{id:"3-full-control-over-instructions",children:"3. Full Control Over Instructions"}),"\n",(0,t.jsx)(n.p,{children:"You can easily provide different system instructions for each turn of the conversation, allowing for dynamic guidance as the conversation evolves."}),"\n",(0,t.jsx)(n.h3,{id:"4-type-safety-and-reliability",children:"4. Type Safety and Reliability"}),"\n",(0,t.jsx)(n.p,{children:"Strong typing via Pydantic models ensures your conversation structures are validated at runtime."}),"\n",(0,t.jsx)(n.h3,{id:"5-provider-agnostic-implementation",children:"5. Provider-Agnostic Implementation"}),"\n",(0,t.jsx)(n.p,{children:"The same code works across all supported providers without modification."}),"\n",(0,t.jsx)(n.h2,{id:"common-use-cases",children:"Common Use Cases"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Chatbots with Memory"}),": Build chatbots that remember previous interactions"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Multi-Stage Processes"}),": Guide users through multi-step workflows"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Dynamic Model Selection"}),": Use cost-effective models for simple queries and more powerful models for complex ones"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Provider Fallback"}),": Switch providers if one is unavailable or rate-limited"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"A/B Testing Models"}),": Compare responses from different models for the same conversation"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Add File Support"}),": Extend the example to include file attachments in conversation turns"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Implement Streaming"}),": Modify the configuration to enable streaming responses"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Add Error Handling"}),": Implement retries and fallbacks for failed requests"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Add Async Support"}),": Convert the example to use async/await for improved performance in server environments"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,t.jsx)(n.p,{children:"Dhenara's multi-turn conversation capabilities provide a powerful foundation for building sophisticated AI interactions with minimal code. The clean, provider-agnostic design allows you to focus on your application logic rather than wrangling different provider APIs."})]})}function p(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}},8453:(e,n,o)=>{o.d(n,{R:()=>s,x:()=>a});var i=o(6540);const t={},r=i.createContext(t);function s(e){const n=i.useContext(r);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:s(e.components),i.createElement(r.Provider,{value:n},e.children)}}}]);