"use strict";(self.webpackChunkdhenara_docs=self.webpackChunkdhenara_docs||[]).push([[466],{3481:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>a,contentTitle:()=>l,default:()=>h,frontMatter:()=>s,metadata:()=>o,toc:()=>d});const o=JSON.parse('{"id":"getting-started/key-concepts","title":"Key Concepts","description":"Understanding the core concepts of Dhenara will help you use the library effectively. This guide explains the fundamental components and how they work together.","source":"@site/docs/getting-started/key-concepts.md","sourceDirName":"getting-started","slug":"/getting-started/key-concepts","permalink":"/getting-started/key-concepts","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"title":"Key Concepts"},"sidebar":"docsSidebar","previous":{"title":"Quick Start","permalink":"/getting-started/quick-start"},"next":{"title":"Why Dhenara","permalink":"/why-dhenara/"}}');var r=i(4848),t=i(8453);const s={title:"Key Concepts"},l="Key Concepts",a={},d=[{value:"Architecture Overview",id:"architecture-overview",level:2},{value:"Core Components",id:"core-components",level:2},{value:"AIModelAPI",id:"aimodelapi",level:3},{value:"AIModel &amp; Foundation Models",id:"aimodel--foundation-models",level:3},{value:"AIModelEndpoint",id:"aimodelendpoint",level:3},{value:"AIModelClient",id:"aimodelclient",level:3},{value:"AIModelCallConfig",id:"aimodelcallconfig",level:3},{value:"Unified Response Format",id:"unified-response-format",level:2},{value:"Working with Streaming",id:"working-with-streaming",level:2},{value:"Typical Workflow",id:"typical-workflow",level:2},{value:"Error Handling and Resource Management",id:"error-handling-and-resource-management",level:2},{value:"Next Steps",id:"next-steps",level:2}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"key-concepts",children:"Key Concepts"})}),"\n",(0,r.jsx)(n.p,{children:"Understanding the core concepts of Dhenara will help you use the library effectively. This guide explains the fundamental components and how they work together."}),"\n",(0,r.jsx)(n.h2,{id:"architecture-overview",children:"Architecture Overview"}),"\n",(0,r.jsx)(n.p,{children:"Dhenara is built on principles of simplicity, flexibility, and separation of concerns. The architecture separates:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"API Providers"})," - The services that expose AI model APIs (OpenAI, Anthropic, Amazon Bedrock, Microsoft Azure etc.)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Models"})," - The specific AI models with their capabilities and parameters"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Endpoints"})," - The combination of an API provider and a specific model"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Clients"})," - The interface you use to interact with endpoints"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"This separation lets you:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Switch between models while keeping the same code structure"}),"\n",(0,r.jsx)(n.li,{children:"Use the same model through different API providers"}),"\n",(0,r.jsx)(n.li,{children:"Configure each component independently"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"core-components",children:"Core Components"}),"\n",(0,r.jsx)(n.h3,{id:"aimodelapi",children:"AIModelAPI"}),"\n",(0,r.jsx)(n.p,{children:"Represents credentials and configuration for a specific AI provider:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# Create API configurations for different providers\nopenai_api = AIModelAPI(\n    provider=AIModelAPIProviderEnum.OPEN_AI,\n    api_key="your_openai_api_key",\n)\n\nanthropic_api = AIModelAPI(\n    provider=AIModelAPIProviderEnum.ANTHROPIC,\n    api_key="your_anthropic_api_key",\n)\n\nvertex_ai_api = AIModelAPI(\n    provider=AIModelAPIProviderEnum.GOOGLE_VERTEX_AI,\n    credentials={"service_account_json": {...}},\n    config={"project_id": "your-project", "location": "us-central1"},\n)\n'})}),"\n",(0,r.jsx)(n.h3,{id:"aimodel--foundation-models",children:"AIModel & Foundation Models"}),"\n",(0,r.jsx)(n.p,{children:"Predefined models with appropriate settings and capabilities:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Token limits"}),"\n",(0,r.jsx)(n.li,{children:"Context window sizes"}),"\n",(0,r.jsx)(n.li,{children:"Cost information"}),"\n",(0,r.jsx)(n.li,{children:"Provider-specific parameters"}),"\n",(0,r.jsx)(n.li,{children:"Model Options ( This is very useful when you deal with image generation )"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Dhenara includes foundation models for popular services like OpenAI's GPT models, Google's Gemini, Anthropic's Claude, DeepSeek's R1 and more."}),"\n",(0,r.jsx)(n.h3,{id:"aimodelendpoint",children:"AIModelEndpoint"}),"\n",(0,r.jsx)(n.p,{children:"Connects a specific model with an API configuration:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# Connect models with API providers\ngpt4o_endpoint = AIModelEndpoint(\n    api=openai_api,\n    ai_model=GPT4o,\n)\n\nclaude_endpoint = AIModelEndpoint(\n    api=anthropic_api,\n    ai_model=Claude37Sonnet,\n)\n"})}),"\n",(0,r.jsx)(n.p,{children:"The same model can be used with different API providers:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# Using Claude through different API providers\nclaude_direct = AIModelEndpoint(api=anthropic_api, ai_model=Claude37Sonnet)\nclaude_on_bedrock = AIModelEndpoint(api=bedrock_api, ai_model=Claude37Sonnet)\nclaude_on_vertex = AIModelEndpoint(api=vertex_ai_api, ai_model=Claude37Sonnet)\n"})}),"\n",(0,r.jsx)(n.h3,{id:"aimodelclient",children:"AIModelClient"}),"\n",(0,r.jsx)(n.p,{children:"The main interface for generating content. It handles:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Connection lifecycle management"}),"\n",(0,r.jsx)(n.li,{children:"Request formatting and validation"}),"\n",(0,r.jsx)(n.li,{children:"Response parsing and normalization"}),"\n",(0,r.jsx)(n.li,{children:"Error handling and retries"}),"\n",(0,r.jsx)(n.li,{children:"Streaming management"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Available in both synchronous and asynchronous modes:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# Synchronous client\nclient = AIModelClient(\n    model_endpoint=endpoint,\n    config=config,\n    is_async=False,\n)\n\n# Asynchronous client\nasync_client = AIModelClient(\n    model_endpoint=endpoint,\n    config=config,\n    is_async=True,\n)\n"})}),"\n",(0,r.jsx)(n.h3,{id:"aimodelcallconfig",children:"AIModelCallConfig"}),"\n",(0,r.jsx)(n.p,{children:"Controls the behavior of individual API calls:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# Text Generation\ncall_config = AIModelCallConfig(\n    max_output_tokens=4000,  # Limit response length\n    streaming=True,          # Enable streaming\n    reasoning=True,          # Enable reasoning/thinking mode\n    max_reasoning_tokens=8000,  # Limit reasoning tokens\n    timeout=30,              # Set timeout in seconds\n    retries=3,               # Configure retries\n    options={},              # Model-specific options\n)\n\n\n#Image call config for Dalle3\ncall_config=AIModelCallConfig(\n    options={\n        "quality": "standard",\n        "size": "1024x1024",\n        "style": "natural",\n        "n": 1,\n        "response_format": "b64_json",\n    },\n)\n'})}),"\n",(0,r.jsx)(n.h2,{id:"unified-response-format",children:"Unified Response Format"}),"\n",(0,r.jsx)(n.p,{children:"Dhenara normalizes responses from all providers into consistent types:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"AIModelCallResponse"}),": Top-level container for all responses"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"ChatResponse"}),": For text generation"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"ImageResponse"}),": For image generation"]}),"\n",(0,r.jsx)(n.li,{children:"Streaming variants with identical structure"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"This allows switching between providers without changing your response handling code."}),"\n",(0,r.jsx)(n.h2,{id:"working-with-streaming",children:"Working with Streaming"}),"\n",(0,r.jsx)(n.p,{children:"Dhenara provides first-class support for streaming responses:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'response = client.generate(prompt=prompt)\n\nif response.stream_generator:\n    for chunk, accumulated in response.stream_generator:\n        if chunk:\n            # Process each token as it arrives\n            print(chunk.data.choice_deltas[0].content_deltas[0].get_text_delta(),\n                  end="", flush=True)\n\n        # On the last iteration, accumulated contains the complete response\n        if accumulated:\n            final_response = accumulated\n'})}),"\n",(0,r.jsx)(n.p,{children:"Streaming responses automatically accumulate content, providing a final response identical in structure to non-streaming responses."}),"\n",(0,r.jsx)(n.h2,{id:"typical-workflow",children:"Typical Workflow"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Configure API credentials"})," - Create ",(0,r.jsx)(n.code,{children:"AIModelAPI"})," instances"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Select and configure models"})," - Choose from foundation models or create custom ones"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Create endpoints"})," - Connect models with API providers"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Configure and create a client"})," - Set up behavior for API calls"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Generate content"})," - Use the client to send prompts and process responses"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"error-handling-and-resource-management",children:"Error Handling and Resource Management"}),"\n",(0,r.jsx)(n.p,{children:"Dhenara automatically manages resources and connections:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# Resources automatically cleaned up when context exits\nwith AIModelClient(...) as client:\n    response = client.generate(prompt)\n\n# Async version\nasync with AIModelClient(...) as client:\n    response = await client.generate_async(prompt)\n"})}),"\n",(0,r.jsx)(n.p,{children:"The library includes built-in error handling, retries, and timeouts to ensure robust operation in production environments."}),"\n",(0,r.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Read the ",(0,r.jsx)(n.a,{href:"./installation",children:"installation guide"})," if you haven't already"]}),"\n",(0,r.jsxs)(n.li,{children:["Try the ",(0,r.jsx)(n.a,{href:"./quick-start",children:"quick start examples"})," to see these concepts in action"]}),"\n",(0,r.jsx)(n.li,{children:"Explore the API reference for detailed information on each component"}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>s,x:()=>l});var o=i(6540);const r={},t=o.createContext(r);function s(e){const n=o.useContext(t);return o.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:s(e.components),o.createElement(t.Provider,{value:n},e.children)}}}]);