"use strict";(self.webpackChunkdhenara_docs=self.webpackChunkdhenara_docs||[]).push([[4623],{8453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>s});var i=t(6540);const a={},o=i.createContext(a);function r(e){const n=i.useContext(o);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),i.createElement(o.Provider,{value:n},e.children)}},9978:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>c,frontMatter:()=>r,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"dhenara-agent/guides/examples/image-agent","title":"Image Generation Agent","description":"This example demonstrates how to create an agent that generates images based on text prompts using Dhenara Agent DSL","source":"@site/docs/dhenara-agent/guides/examples/image-agent.md","sourceDirName":"dhenara-agent/guides/examples","slug":"/dhenara-agent/guides/examples/image-agent","permalink":"/dhenara-agent/guides/examples/image-agent","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"sidebar_position":5},"sidebar":"dhenaraAgentSidebar","previous":{"title":"Simple Chatbot","permalink":"/dhenara-agent/guides/examples/simple-chatbot"},"next":{"title":"Architecture Overview","permalink":"/dhenara-agent/architecture/overview"}}');var a=t(4848),o=t(8453);const r={sidebar_position:5},s="Image Generation Agent",l={},d=[{value:"Agent Overview",id:"agent-overview",level:2},{value:"Agent Structure",id:"agent-structure",level:2},{value:"Agent Definition",id:"agent-definition",level:2},{value:"Data Models",id:"data-models",level:2},{value:"Image Generation Flow",id:"image-generation-flow",level:2},{value:"Coordinator Flow",id:"coordinator-flow",level:2},{value:"Event Handler",id:"event-handler",level:2},{value:"Image Task JSON",id:"image-task-json",level:2},{value:"Running the Agent",id:"running-the-agent",level:2},{value:"Output and Artifacts",id:"output-and-artifacts",level:2},{value:"Customization Options",id:"customization-options",level:2},{value:"Conclusion",id:"conclusion",level:2}];function g(e){const n={code:"code",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"image-generation-agent",children:"Image Generation Agent"})}),"\n",(0,a.jsx)(n.p,{children:"This example demonstrates how to create an agent that generates images based on text prompts using Dhenara Agent DSL\n(DAD). The Image Generation Agent showcases how DAD can integrate with different AI modalities beyond text."}),"\n",(0,a.jsx)(n.h2,{id:"agent-overview",children:"Agent Overview"}),"\n",(0,a.jsx)(n.p,{children:"The Image Generation Agent can:"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"Accept text prompts defining the images to generate"}),"\n",(0,a.jsx)(n.li,{children:"Select appropriate image generation models"}),"\n",(0,a.jsx)(n.li,{children:"Generate multiple images with different parameters"}),"\n",(0,a.jsx)(n.li,{children:"Save the generated images to a specified directory"}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"This example demonstrates integrating with image generation models like DALL-E, Imagen, and Midjourney through their\nrespective API interfaces."}),"\n",(0,a.jsx)(n.h2,{id:"agent-structure",children:"Agent Structure"}),"\n",(0,a.jsx)(n.p,{children:"The Image Generation Agent consists of the following components:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-plaintext",children:"src/agents/image_gen/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 agent.py           # Main agent definition\n\u251c\u2500\u2500 handler.py         # Event handlers\n\u2514\u2500\u2500 flows/             # Flow definitions\n    \u251c\u2500\u2500 coordinator.py     # Main coordinator flow\n    \u2514\u2500\u2500 image_generator.py # Image generation flow\n"})}),"\n",(0,a.jsx)(n.h2,{id:"agent-definition",children:"Agent Definition"}),"\n",(0,a.jsx)(n.p,{children:"The main agent definition connects the coordinator flow:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from dhenara.agent.dsl import AgentDefinition\n\nfrom .flows.coordinator import coordinator_flow\n\nagent = AgentDefinition()\nagent.flow(\n    "imagegen_flow",\n    coordinator_flow,\n)\n'})}),"\n",(0,a.jsx)(n.h2,{id:"data-models",children:"Data Models"}),"\n",(0,a.jsx)(n.p,{children:"The agent uses structured data models to define image generation tasks:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"from dhenara.agent.types import BaseModel\n\nclass ImageTaskSpec(BaseModel):\n    filename: str   # Base filename for the generated image\n    prompt: str     # Text prompt describing the image to generate\n\nclass ImageTask(BaseModel):\n    task_specifications: list[ImageTaskSpec]  # List of image specs to generate\n"})}),"\n",(0,a.jsx)(n.h2,{id:"image-generation-flow",children:"Image Generation Flow"}),"\n",(0,a.jsx)(n.p,{children:"The core image generation flow handles single image generation:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from dhenara.agent.dsl import (\n    PLACEHOLDER, AIModelNode, AIModelNodeSettings, EventType, FlowDefinition\n)\nfrom dhenara.ai.types import AIModelCallConfig, Prompt\n\n# Define available models with their parameters\nmodels_with_options = {\n    "gpt-image-1": {\n        "quality": "medium",\n        "size": "1536x1024",\n        "n": 2,\n    },\n    "imagen-3.0-generate": {\n        "aspect_ratio": "16:9",\n        "number_of_images": 1,\n        "person_generation": "dont_allow",\n    },\n    "imagen-3.0-fast-generate": {\n        "aspect_ratio": "16:9",\n        "number_of_images": 1,\n        "person_generation": "dont_allow",\n    },\n}\n\n# Image directory configuration\nglobal_image_directory = "$var{run_root}/global_images"\n\n# Single image generation flow\nsingle_img_flow = FlowDefinition()\nsingle_img_flow.vars(\n    {\n        "task_spec": PLACEHOLDER,  # Will be populated by the multi-image flow\n    }\n)\n\n# Image generation node\nsingle_img_flow.node(\n    "image_generator",\n    AIModelNode(\n        models=list(models_with_options.keys()),\n        pre_events=[EventType.node_input_required],\n        settings=AIModelNodeSettings(\n            system_instructions=[\n                "You are a professional image generation agent specialized in executing precise image operations.",\n            ],\n            prompt=Prompt.with_dad_text("$expr{task_spec.prompt}. \\nCreate image\\n"),\n            model_call_config=AIModelCallConfig(\n                test_mode=False,\n                options={},  # Options will be set by the model handler\n            ),\n            save_generated_bytes=True,  # Save the images\n            bytes_save_path=global_image_directory,\n            bytes_save_filename_prefix="$expr{task_spec.filename}",\n        ),\n    ),\n)\n\n# Multi-image flow for batch processing\nmulti_image_flow = FlowDefinition()\nmulti_image_flow.vars(\n    {\n        "image_task": PLACEHOLDER,  # Will be populated by the coordinator\n    }\n)\n\n# Process each image task specification\nmulti_image_flow.for_each(\n    id="image_gen_loop",\n    statement="$expr{image_task.task_specifications}",\n    item_var="task_spec",\n    max_iterations=20,\n    body=single_img_flow,\n)\n'})}),"\n",(0,a.jsx)(n.h2,{id:"coordinator-flow",children:"Coordinator Flow"}),"\n",(0,a.jsx)(n.p,{children:"The coordinator flow manages the overall process, including task planning and execution:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from dhenara.agent.dsl import FlowDefinition\nfrom dhenara.agent.dsl.inbuilt.flow_nodes.command import CommandNode, CommandNodeSettings\nfrom dhenara.ai.types import ObjectTemplate\n\nfrom ...autocoder.flows.planner import planner_flow\nfrom .image_generator import multi_image_flow\n\n# Load task information\ntask_background = read_background()\ntask_description = read_description()\n\n# Coordinator flow definition\ncoordinator_flow = FlowDefinition()\n\n# First, run the planner to generate the image task\ncoordinator_flow.subflow(\n    "planner",\n    planner_flow,\n    variables={\n        "task_background": task_background,\n        "task_description": task_description,\n    },\n)\n\n# Based on planning results, generate images\ncoordinator_flow.conditional(\n    id="plan_executor",\n    statement=ObjectTemplate(\n        expression="$expr{py: $hier{planner.plan_generator}.outcome.structured is not None}",\n    ),\n    true_branch=multi_image_flow,\n    true_branch_variables={\n        "image_task": "$expr{$hier{planner.plan_generator}.outcome.structured}",\n    },\n    false_branch=FlowDefinition().node(\n        "no_plan_generated",\n        CommandNode(\n            settings=CommandNodeSettings(\n                commands=["echo \'Planner is unsuccessful.\'"],\n            )\n        ),\n    ),\n)\n'})}),"\n",(0,a.jsx)(n.h2,{id:"event-handler",children:"Event Handler"}),"\n",(0,a.jsx)(n.p,{children:"The event handler manages input for the AI model nodes, including model selection and options:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from dhenara.agent.dsl import FlowNodeTypeEnum, NodeInputRequiredEvent\nfrom dhenara.agent.utils.helpers.terminal import get_ai_model_node_input\n\nfrom .flows.image_generator import ImageTask, models_with_options\n\nasync def image_gen_input_handler(event: NodeInputRequiredEvent):\n    if event.node_type == FlowNodeTypeEnum.ai_model_call:\n        node_input = None\n\n        if event.node_id == "plan_generator":\n            # For the planner, select from text generation models\n            node_input = await get_ai_model_node_input(\n                node_def_settings=event.node_def_settings,\n                models=plan_gen_models,\n                models_with_options=None,\n                enable_option_update=False,\n                structured_output=ImageTask,\n            )\n\n        elif event.node_id == "image_generator":\n            # For image generation, select from image models with options\n            node_input = await get_ai_model_node_input(\n                node_def_settings=event.node_def_settings,\n                models=None,\n                models_with_options=models_with_options,\n                enable_option_update=True,  # Allow updating model options\n                structured_output=None,\n            )\n\n        else:\n            print(f"WARNING: Unhandled ai_model_call input event for node {event.node_id}")\n\n        event.input = node_input\n        event.handled = True\n'})}),"\n",(0,a.jsx)(n.h2,{id:"image-task-json",children:"Image Task JSON"}),"\n",(0,a.jsx)(n.p,{children:"You can define image tasks in a JSON file for direct loading:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-json",children:'{\n  "task_specifications": [\n    {\n      "filename": "sunset_beach",\n      "prompt": "A breathtaking sunset over a tropical beach with silhouettes of palm trees"\n    },\n    {\n      "filename": "mountain_cabin",\n      "prompt": "A cozy cabin in snow-covered mountains with smoke coming from the chimney"\n    },\n    {\n      "filename": "futuristic_city",\n      "prompt": "A futuristic city skyline with flying cars and holographic advertisements"\n    }\n  ]\n}\n'})}),"\n",(0,a.jsx)(n.h2,{id:"running-the-agent",children:"Running the Agent"}),"\n",(0,a.jsx)(n.p,{children:"To run the Image Generation Agent:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from dhenara.agent.dsl.events import EventType\nfrom dhenara.agent.run import RunContext\nfrom dhenara.agent.runner import AgentRunner\n\nfrom src.agents.image_gen.agent import agent\nfrom src.agents.image_gen.handler import image_gen_input_handler\nfrom src.runners.defs import observability_settings, project_root\n\nroot_component_id = "image_gen_root"\nagent.root_id = root_component_id\n\nrun_context = RunContext(\n    root_component_id=root_component_id,\n    observability_settings=observability_settings,\n    project_root=project_root,\n    run_root_subpath="agent_image_gen",\n)\n\nrun_context.register_event_handlers(\n    handlers_map={\n        EventType.node_input_required: image_gen_input_handler,\n        # Additional event handlers...\n    }\n)\n\nrunner = AgentRunner(agent, run_context)\n'})}),"\n",(0,a.jsx)(n.h2,{id:"output-and-artifacts",children:"Output and Artifacts"}),"\n",(0,a.jsx)(n.p,{children:"After running the agent, you'll find:"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:["Generated images saved in the ",(0,a.jsx)(n.code,{children:"global_images"})," directory with the specified filenames"]}),"\n",(0,a.jsx)(n.li,{children:"Run artifacts showing the processing steps and model outputs"}),"\n",(0,a.jsx)(n.li,{children:"Context information in the run directory for debugging or analysis"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"customization-options",children:"Customization Options"}),"\n",(0,a.jsx)(n.p,{children:"You can customize this agent in several ways:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Model Selection"}),": Add or modify the available image generation models"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Model Parameters"}),": Adjust quality, size, and other model-specific parameters"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Prompt Engineering"}),": Enhance prompt templates for better image generation"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Post-Processing"}),": Add nodes for image processing after generation"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Batching Logic"}),": Modify how image generation tasks are batched and processed"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,a.jsx)(n.p,{children:"The Image Generation Agent demonstrates how DAD can integrate with image generation models to create a flexible and\npowerful workflow. This example showcases key DAD features like:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Multi-modal AI Integration"}),": Working with both text and image models"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Structured Data Handling"}),": Using Pydantic models for task specifications"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Parameter Customization"}),": Configuring model-specific parameters"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Artifact Management"}),": Saving generated binary data (images)"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Batch Processing"}),": Processing multiple image generation tasks"]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"By understanding this example, you can create your own specialized agents that leverage visual AI capabilities within\nthe DAD framework."})]})}function c(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(g,{...e})}):g(e)}}}]);