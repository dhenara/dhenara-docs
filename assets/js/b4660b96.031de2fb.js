"use strict";(self.webpackChunkdhenara_docs=self.webpackChunkdhenara_docs||[]).push([[182],{431:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>d,contentTitle:()=>i,default:()=>l,frontMatter:()=>a,metadata:()=>r,toc:()=>p});const r=JSON.parse('{"id":"guides/prompt-formatter","title":"Prompt Formatter","description":"One onf the challenge in using models from multiple provider is , they all need prompts in different format. Eg \'assistant\'} or  {\'role\': \'system\'} . Dhenara makes this convertion easier by using the prompt formatter.","source":"@site/docs/guides/prompt-formatter.md","sourceDirName":"guides","slug":"/guides/prompt-formatter","permalink":"/guides/prompt-formatter","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"title":"Prompt Formatter"}}');var t=o(4848),s=o(8453);const a={title:"Prompt Formatter"},i="PromptFormatter /Prompt Convertor",d={},p=[];function m(e){const n={code:"code",h1:"h1",header:"header",p:"p",pre:"pre",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"promptformatter-prompt-convertor",children:"PromptFormatter /Prompt Convertor"})}),"\n",(0,t.jsxs)(n.p,{children:["One onf the challenge in using models from multiple provider is , they all need prompts in different format. Eg: to represent previoously generated content, the format could be ",(0,t.jsx)(n.code,{children:" {'role': 'assistant'}"})," or ",(0,t.jsx)(n.code,{children:" {'role': 'system'}"})," . Dhenara makes this convertion easier by using the prompt formatter."]}),"\n",(0,t.jsx)(n.p,{children:"For a specific nodei"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"from dhenara.ai.providers.common import PromptFormatter\n\nprompts = PromptFormatter.format_conversion_node_as_prompts(\n            model=destination_model,\n            user_query=self.question,\n            attached_files=attached_files,\n            previous_response=previous_response,\n            max_words_query=max_words_question,\n            max_words_files=max_words_files,\n            max_words_response=max_words_answer,\n            # NOTE: As we reverse the order get_ancestor_prompts, add answer before question\n            response_before_query=True,  # NOTE: True. see comments above\n        )\n"})}),"\n",(0,t.jsxs)(n.p,{children:["This will return an array of of prompts that is in the format of the ",(0,t.jsx)(n.code,{children:"model"}),".\nThis  is very useful if you want to send a node ( a user inut and the output generated by a model) as input to a model from a different provider (Say, form open-ai to anthropic).\nYou will use it like"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'def main():\n    model_endpoint = AIModelEndpoint() # Setup model endpoint correctly\n\n    destination_model = model_endpoint.ai_model\n\n    user_query= "Tell me a joke."\n\n    # Currently there is no Pydantic model defined to represent a Node. We will add this going forward\n    previous_nodes.append( {\n        "user_query": user_query,\n        "attached_files": [],\n        "response": response.chat_response,\n    })\n\n\n    prompt = PromptFormatter.format_conversion_node_as_prompts(\n            model=destination_model,\n            user_query=user_query,\n            attached_files=[],\n            previous_response=[], # Don\'t add previous responses here, as it will be passed as the `context`\n        )\n\n    context = get_context(previous_nodes)\n\n\n\ndef get_context( previouse_nodes: dict, destination_model: AIModel,):\n    context= []\n\n    for node  in nodes:\n        prompts = PromptFormatter.format_conversion_node_as_prompts(\n            model=destination_model,\n            user_query=node["user_query"],\n            attached_files=node["attached_files"],\n            previous_response=node["response"],\n            # Optionally you cann limit number of workds in question, and pevious response\n            max_words_query=None ,\n            max_words_files=None,\n            max_words_response=None,\n            # You can chage the order in which question and anser promts are appended\n            response_before_query=True,\n        )\n        context.extend(prompts)\n\n    return context\n\n'})})]})}function l(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(m,{...e})}):m(e)}},8453:(e,n,o)=>{o.d(n,{R:()=>a,x:()=>i});var r=o(6540);const t={},s=r.createContext(t);function a(e){const n=r.useContext(s);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),r.createElement(s.Provider,{value:n},e.children)}}}]);