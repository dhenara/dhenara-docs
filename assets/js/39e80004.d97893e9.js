"use strict";(self.webpackChunkdhenara_docs=self.webpackChunkdhenara_docs||[]).push([[520],{1164:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>p,contentTitle:()=>i,default:()=>d,frontMatter:()=>a,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"samples/text-gen/streaming","title":"Streaming","description":"Below example shows streaming chat response.","source":"@site/docs/samples/text-gen/streaming.md","sourceDirName":"samples/text-gen","slug":"/samples/text-gen/streaming","permalink":"/samples/text-gen/streaming","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"title":"Streaming"},"sidebar":"docsSidebar","previous":{"title":"Async Text Generation","permalink":"/samples/text-gen/text-gen-async"},"next":{"title":"Async Streaming","permalink":"/samples/text-gen/streaming-async"}}');var r=t(4848),o=t(8453);const a={title:"Streaming"},i="Streaming",p={},c=[];function l(n){const e={code:"code",h1:"h1",header:"header",p:"p",pre:"pre",...(0,o.R)(),...n.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(e.header,{children:(0,r.jsx)(e.h1,{id:"streaming",children:"Streaming"})}),"\n",(0,r.jsx)(e.p,{children:"Below example shows streaming chat response."}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'from dhenara.ai import AIModelClient\nfrom dhenara.ai.types import AIModelCallConfig, AIModelEndpoint\nfrom dhenara.ai.types.external_api import AIModelAPIProviderEnum\nfrom dhenara.ai.types.genai import AIModelAPI, ChatResponseChunk\nfrom dhenara.ai.types.genai.foundation_models.anthropic.chat import Claude37Sonnet\nfrom dhenara.ai.types.shared import SSEErrorResponse, SSEEventType, SSEResponse\n\n# 1. Create an API\n# This can be used to create multiple model endpoints for the same API provider\napi = AIModelAPI(\n    provider=AIModelAPIProviderEnum.ANTHROPIC,\n    api_key="your_api_key",\n)\n\n# 2. Select or create an AI model\n# You can either use the foundation models as it is, or create your own models\nmodel = Claude37Sonnet\n\n# Create the model endpoint\nmodel_endpoint = AIModelEndpoint(api=api, ai_model=model)\n\n# Create the client\nclient = AIModelClient(\n    model_endpoint=model_endpoint,\n    config=AIModelCallConfig(\n        max_output_tokens=16000,\n        reasoning=True,  # thinking/reasoning mode\n        max_reasoning_tokens=8000,  # Needed only if reasoning is set\n        streaming=True,\n    ),\n    is_async=False,  # Sync mode/ async mode\n)\n\n\nresponse = client.generate(\n    prompt={\n        "role": "user",\n        "content": "Explain quantum computing?",\n    },\n    context=[],\n    instructions=[],\n)\n\nprint(response)\n\n\n# -----------------------------------------------------------------------------\nclass StreamProcesor:\n    def __init__(self):\n        super().__init__()\n        self.previous_content_delta = None\n\n    def process_stream_response(self, response):\n        print("\\nAssistant: ", end="", flush=True)\n\n        try:\n            for chunk, final_response in response.stream_generator:\n                if chunk:\n                    if isinstance(chunk, SSEErrorResponse):\n                        self.print_error(f"{chunk.data.error_code}: {chunk.data.message}")\n                        break\n\n                    if not isinstance(chunk, SSEResponse):\n                        self.print_error(f"Unknown type {type(chunk)}")\n                        continue\n\n                    if chunk.event == SSEEventType.ERROR:\n                        self.print_error(f"Stream Error: {chunk}")\n                        break\n\n                    if chunk.event == SSEEventType.TOKEN_STREAM:\n                        self.print_stream_chunk(chunk.data)\n                        if chunk.data.done:\n                            # Don\'t `break` as final response will be send after this\n                            pass\n\n                if final_response:\n                    # THe best part is that, you will get the consolidated response\n                    # in the same format as you were receiving for a non-streaming case.\n                    # You don\'t need to manage accumulating the response in different formats\n                    self.print_final_response(final_response)\n        except KeyboardInterrupt:\n            self.print_warning("Stream interrupted by user")\n        except Exception as e:\n            self.print_error(f"Error processing stream: {e!s}")\n        finally:\n            print("\\n")\n\n    # For streaming responses\n    def print_stream_chunk(self, chunk: ChatResponseChunk):\n        """Print the content from a stream chunk"""\n        for choice_delta in chunk.choice_deltas:\n            if not choice_delta.content_deltas:\n                continue\n\n            for content_delta in choice_delta.content_deltas:\n                same_content = (\n                    self.previous_content_delta\n                    and self.previous_content_delta.index == content_delta.index\n                    and self.previous_content_delta.type == content_delta.type\n                )\n                if not same_content:\n                    self.previous_content_delta = content_delta\n                    print()\n                    print("-" * 80)\n                    print(f"\\n[{content_delta.type}:]")\n                    print("-" * 80)\n\n                # Actual content\n                text = content_delta.get_text_delta()\n                if text:\n                    print(f"{text}", end="", flush=True)\n\n    def print_error(self, message: str):\n        print(f"\\nError: {message}")\n\n    # This fn is same as the print_response() fn for non streaming case.\n    # You will get a consolidated response as the last respone even for streaming\n    def print_final_response(self, response):\n        print()\n        print("~" * 80)\n        print("Streaming Finished. Final Consolidated Response is\\n")\n        print("~" * 80)\n        for choice in response.chat_response.choices:\n            for content in choice.contents:\n                # Some formatting to differentiate contents\n                # With reasoning=True, same response will have multiple contents\n                print()\n                print("-" * 80)\n                print(f"Type:: {content.type}")\n                print("-" * 80)\n                # Actual response text\n                print(f"{content.get_text()}")\n\n        # Optionally get the usage and cost for this call.\n        # Usage/Cost calculation is enabled by default, but can be disabled via setting\n        print("-" * 80)\n        print(f"Usage: {response.chat_response.usage}")\n        print(f"Usage Charge: {response.chat_response.usage_charge}")\n        print("-" * 80)\n\n\n# -----------------------------------------------------------------------------\n\nstream_procesor = StreamProcesor()\nstream_procesor.process_stream_response(response)\n\n'})})]})}function d(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,r.jsx)(e,{...n,children:(0,r.jsx)(l,{...n})}):l(n)}},8453:(n,e,t)=>{t.d(e,{R:()=>a,x:()=>i});var s=t(6540);const r={},o=s.createContext(r);function a(n){const e=s.useContext(o);return s.useMemo((function(){return"function"==typeof n?n(e):{...e,...n}}),[e,n])}function i(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(r):n.components||r:a(n.components),s.createElement(o.Provider,{value:e},n.children)}}}]);