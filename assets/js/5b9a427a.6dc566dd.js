"use strict";(self.webpackChunkdhenara_docs=self.webpackChunkdhenara_docs||[]).push([[8274],{8453:(e,n,o)=>{o.d(n,{R:()=>s,x:()=>a});var t=o(6540);const i={},r=t.createContext(i);function s(e){const n=t.useContext(r);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),t.createElement(r.Provider,{value:n},e.children)}},9205:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>d,contentTitle:()=>a,default:()=>p,frontMatter:()=>s,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"dhenara-ai/features/multi-turn-conversations","title":"Multi-Turn Conversations","description":"Multi-Turn Conversations with Dhenara","source":"@site/docs/dhenara-ai/features/multi-turn-conversations.md","sourceDirName":"dhenara-ai/features","slug":"/dhenara-ai/features/multi-turn-conversations","permalink":"/dhenara-ai/features/multi-turn-conversations","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"title":"Multi-Turn Conversations"},"sidebar":"dhenaraAiSidebar","previous":{"title":"Overview","permalink":"/dhenara-ai/features/features-overview"},"next":{"title":"Resource Configuration","permalink":"/dhenara-ai/features/resource-configuration"}}');var i=o(4848),r=o(8453);const s={title:"Multi-Turn Conversations"},a=void 0,d={},l=[{value:"Multi-Turn Conversations with Dhenara",id:"multi-turn-conversations-with-dhenara",level:2},{value:"Real World Usage",id:"real-world-usage",level:2},{value:"How It Works",id:"how-it-works",level:2},{value:"Key Components",id:"key-components",level:3},{value:"Key Benefits",id:"key-benefits",level:2},{value:"When to Use This Pattern",id:"when-to-use-this-pattern",level:2}];function c(e){const n={code:"code",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.h2,{id:"multi-turn-conversations-with-dhenara",children:"Multi-Turn Conversations with Dhenara"}),"\n",(0,i.jsx)(n.p,{children:"One of Dhenara's most powerful features is its ability to seamlessly manage multi-turn conversations with AI models. The\ncode example demonstrates how to create coherent, context-aware conversations across multiple turns, even when switching\nbetween different AI models and providers."}),"\n",(0,i.jsx)(n.p,{children:"Multi-turn conversations allow your application to maintain context across separate interactions with AI models. This is\nessential for creating natural dialogue flows, chatbots, interactive assistants, and any application where conversation\nhistory matters."}),"\n",(0,i.jsx)(n.h2,{id:"real-world-usage",children:"Real World Usage"}),"\n",(0,i.jsx)(n.p,{children:"In this example, a developer is building an application that requires conversational memory while also having the\nflexibility to route generated outputs between models from different providers."}),"\n",(0,i.jsx)(n.p,{children:"Traditional frameworks often make this challenging, but Dhenara provides a streamlined solution. The code below\ndemonstrates this approach, showcasing how Dhenara manages this complexity seamlessly."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import datetime\nimport random\nfrom typing import Any\n\nfrom dhenara.ai import AIModelClient\nfrom dhenara.ai.providers.common.prompt_formatter import PromptFormatter\nfrom dhenara.ai.types import AIModelCallConfig, AIModelEndpoint\nfrom dhenara.ai.types.conversation._node import ConversationNode\nfrom dhenara.ai.types.external_api import AIModelAPIProviderEnum\nfrom dhenara.ai.types.genai import AIModelAPI\nfrom dhenara.ai.types.genai.foundation_models.anthropic.chat import Claude35Haiku, Claude37Sonnet\nfrom dhenara.ai.types.genai.foundation_models.google.chat import Gemini20Flash, Gemini20FlashLite\nfrom dhenara.ai.types.genai.foundation_models.openai.chat import GPT4oMini, O3Mini\n\n# Initialize API configurations\nanthropic_api = AIModelAPI(\n    provider=AIModelAPIProviderEnum.ANTHROPIC,\n    api_key="your_anthropic_api_key",\n)\nopenai_api = AIModelAPI(\n    provider=AIModelAPIProviderEnum.OPEN_AI,\n    api_key="your_openai_api_key",\n)\ngoogle_api = AIModelAPI(\n    provider=AIModelAPIProviderEnum.GOOGLE_AI,\n    api_key="your_google_api_key",\n)\n\n\n# Create various model endpoints\nall_model_endpoints = [\n    AIModelEndpoint(api=anthropic_api, ai_model=Claude37Sonnet),\n    AIModelEndpoint(api=anthropic_api, ai_model=Claude35Haiku),\n    AIModelEndpoint(api=openai_api, ai_model=O3Mini),\n    AIModelEndpoint(api=openai_api, ai_model=GPT4oMini),\n    AIModelEndpoint(api=google_api, ai_model=Gemini20Flash),\n    AIModelEndpoint(api=google_api, ai_model=Gemini20FlashLite),\n]\n\n\ndef get_context(previous_nodes: list[ConversationNode], destination_model: Any) -> list[Any]:\n    """Process previous conversation nodes into context for the next turn."""\n    context = []\n\n    for node in previous_nodes:\n        prompts = PromptFormatter.format_conversion_node_as_prompts(\n            model=destination_model,\n            user_query=node.user_query,\n            attached_files=node.attached_files,\n            previous_response=node.response,\n        )\n        context.extend(prompts)\n\n    return context\n\n\ndef handle_conversation_turn(\n    user_query: str,\n    instructions: list[str],\n    endpoint: AIModelEndpoint,\n    conversation_nodes: list[ConversationNode],\n) -> ConversationNode:\n    """Process a single conversation turn with the specified model and query."""\n\n    client = AIModelClient(\n        model_endpoint=endpoint,\n        config=AIModelCallConfig(\n            max_output_tokens=1000,\n            streaming=False,\n        ),\n        is_async=False,\n    )\n\n    # Format the user query\n    prompt = PromptFormatter.format_conversion_node_as_prompts(\n        model=endpoint.ai_model,\n        user_query=user_query,\n        attached_files=[],\n        previous_response=[],\n    )[0]\n\n    # Get context from previous turns (if any)\n    context = get_context(conversation_nodes, endpoint.ai_model) if conversation_nodes else []\n\n    # Generate response\n    response = client.generate(\n        prompt=prompt,\n        context=context,\n        instructions=instructions,\n    )\n\n    # Create conversation node\n    node = ConversationNode(\n        user_query=user_query,\n        attached_files=[],\n        response=response.chat_response,\n        timestamp=datetime.datetime.now().isoformat(),\n    )\n\n    return node\n\n\ndef run_multi_turn_conversation():\n    multi_turn_queries = [\n        "Tell me a short story about a robot learning to paint.",\n        "Continue the story but add a twist where the robot discovers something unexpected.",\n        "Conclude the story with an inspiring ending.",\n    ]\n\n    # Instructions for each turn\n    instructions_by_turn = [\n        ["Be creative and engaging."],\n        ["Build upon the previous story seamlessly."],\n        ["Bring the story to a satisfying conclusion."],\n    ]\n\n    # Store conversation history\n    conversation_nodes = []\n\n    # Process each turn\n    for i, query in enumerate(multi_turn_queries):\n        # Choose a random model endpoint\n        model_endpoint = random.choice(all_model_endpoints)\n        # OR choose if fixed order as\n        # model_endpoint = all_model_endpoints[i]\n\n        print(f"\ud83d\udd04 Turn {i + 1} with {model_endpoint.ai_model.model_name} from {model_endpoint.api.provider}\\n")\n\n        node = handle_conversation_turn(\n            user_query=query,\n            instructions=instructions_by_turn[i],  # Only if you need to change instruction on each turn, else leave []\n            endpoint=model_endpoint,\n            conversation_nodes=conversation_nodes,\n        )\n\n        # Display the conversation\n        print(f"User: {query}")\n        print(f"Model: {model_endpoint.ai_model.model_name}\\n")\n        print(f"Model Response:\\n {node.response.choices[0].contents[0].get_text()}\\n")\n        print("-" * 80)\n\n        # Append to nodes, so that next turn will have the context generated\n        conversation_nodes.append(node)\n\n\nif __name__ == "__main__":\n    run_multi_turn_conversation()\n'})}),"\n",(0,i.jsx)(n.h2,{id:"how-it-works",children:"How It Works"}),"\n",(0,i.jsxs)(n.p,{children:["Dhenara uses a simple yet powerful system to manage conversation state through ",(0,i.jsx)(n.code,{children:"ConversationNode"})," objects, which store\neach turn's query, response, and metadata. The ",(0,i.jsx)(n.code,{children:"PromptFormatter"})," handles the conversion of conversation history into\nformats appropriate for each provider."]}),"\n",(0,i.jsx)(n.h3,{id:"key-components",children:"Key Components"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"API Configuration"}),": The example initializes connections to three major providers:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'anthropic_api = AIModelAPI(provider=AIModelAPIProviderEnum.ANTHROPIC, api_key="your_anthropic_api_key")\nopenai_api = AIModelAPI(provider=AIModelAPIProviderEnum.OPEN_AI, api_key="your_openai_api_key")\ngoogle_api = AIModelAPI(provider=AIModelAPIProviderEnum.GOOGLE_AI, api_key="your_google_api_key")\n'})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Model Endpoints"}),": Multiple model endpoints are created, giving you flexibility to use any model from any provider:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"all_model_endpoints = [\n    AIModelEndpoint(api=anthropic_api, ai_model=Claude37Sonnet),\n    AIModelEndpoint(api=openai_api, ai_model=O3Mini),\n    # ...and more\n]\n"})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Context Management"}),": The ",(0,i.jsx)(n.code,{children:"get_context()"})," function processes previous conversation turns into appropriate context\nfor the next model:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"def get_context(previous_nodes: list[ConversationNode], destination_model: Any) -> list[Any]:\n    context = []\n    for node in previous_nodes:\n        prompts = PromptFormatter.format_conversion_node_as_prompts(\n            model=destination_model,\n            user_query=node.user_query,\n            attached_files=node.attached_files,\n            previous_response=node.response,\n        )\n        context.extend(prompts)\n    return context\n"})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Conversation Turn Handler"}),": The ",(0,i.jsx)(n.code,{children:"handle_conversation_turn()"})," function manages a single conversation turn:"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Creates an ",(0,i.jsx)(n.code,{children:"AIModelClient"})," for the current model endpoint"]}),"\n",(0,i.jsx)(n.li,{children:"Formats the user query for the specific model"}),"\n",(0,i.jsx)(n.li,{children:"Obtains context from previous conversation turns"}),"\n",(0,i.jsx)(n.li,{children:"Generates a response"}),"\n",(0,i.jsxs)(n.li,{children:["Creates a ",(0,i.jsx)(n.code,{children:"ConversationNode"})," to store the turn"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Multi-Turn Execution"}),": The ",(0,i.jsx)(n.code,{children:"run_multi_turn_conversation()"})," function demonstrates:"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Managing a series of related queries"}),"\n",(0,i.jsx)(n.li,{children:"Randomly selecting different models for each turn (you could also use a fixed sequence)"}),"\n",(0,i.jsx)(n.li,{children:"Maintaining conversation context between turns"}),"\n",(0,i.jsx)(n.li,{children:"Providing turn-specific instructions to each model"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"key-benefits",children:"Key Benefits"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Model Flexibility"}),": You can switch between models from different providers without losing context."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Provider Abstraction"}),": The same code works with OpenAI, Anthropic, Google, and other providers."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Seamless Context Transfer"}),": Previous conversation turns are automatically converted to the appropriate format for\neach model."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Simple API"}),": Despite the complexity of managing different providers and models, the API remains simple and\nconsistent."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"when-to-use-this-pattern",children:"When to Use This Pattern"}),"\n",(0,i.jsx)(n.p,{children:"This pattern is ideal for:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Building chatbots that need access to multiple models"}),"\n",(0,i.jsx)(n.li,{children:"Creating applications that need to switch models based on specialized capabilities"}),"\n",(0,i.jsx)(n.li,{children:"Implementing failover between different AI providers"}),"\n",(0,i.jsx)(n.li,{children:"A/B testing different models within the same conversation flow"}),"\n",(0,i.jsx)(n.li,{children:"Building systems where different parts of a conversation require different model strengths"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Dhenara's conversation handling makes all these scenarios straightforward to implement while keeping your codebase clean\nand maintainable."})]})}function p(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}}}]);