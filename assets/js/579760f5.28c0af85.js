"use strict";(self.webpackChunkdhenara_docs=self.webpackChunkdhenara_docs||[]).push([[406],{7800:(n,e,o)=>{o.r(e),o.d(e,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>a,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"features/multi-turn/code","title":"Real World Usage","description":"Why Dhenara? We think we should answer this with a real world scenario.","source":"@site/docs/features/multi-turn/code.md","sourceDirName":"features/multi-turn","slug":"/features/multi-turn/code","permalink":"/features/multi-turn/code","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"title":"Real World Usage"},"sidebar":"docsSidebar","previous":{"title":"Overview","permalink":"/features/features-overview"},"next":{"title":"Code Explained","permalink":"/features/multi-turn/description"}}');var i=o(4848),r=o(8453);const a={title:"Real World Usage"},s=void 0,d={},l=[];function p(n){const e={code:"code",p:"p",pre:"pre",...(0,r.R)(),...n.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(e.p,{children:"Why Dhenara? We think we should answer this with a real world scenario.\nHere the developer wants to build an application that has memory /that remembers previous conversaion.\nAt the same time, she wants the flexibility of sending output geneated from one model to another one from a differnet provider."}),"\n",(0,i.jsx)(e.p,{children:"This would make like complicated in all existing framewors.  Below is how Dhenara handles it for you."}),"\n",(0,i.jsx)(e.p,{children:"Here we will just show the code. This will be explained in the next page."}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'import datetime\nimport random\nfrom typing import Any\n\nfrom dhenara.ai import AIModelClient\nfrom dhenara.ai.providers.common.prompt_formatter import PromptFormatter\nfrom dhenara.ai.types import AIModelCallConfig, AIModelEndpoint\nfrom dhenara.ai.types.conversation._node import ConversationNode\nfrom dhenara.ai.types.external_api import AIModelAPIProviderEnum\nfrom dhenara.ai.types.genai import AIModelAPI\nfrom dhenara.ai.types.genai.foundation_models.anthropic.chat import Claude35Haiku, Claude37Sonnet\nfrom dhenara.ai.types.genai.foundation_models.google.chat import Gemini20Flash, Gemini20FlashLite\nfrom dhenara.ai.types.genai.foundation_models.openai.chat import GPT4oMini, O3Mini\n\n# Initialize API configurations\nanthropic_api = AIModelAPI(\n    provider=AIModelAPIProviderEnum.ANTHROPIC,\n    api_key="your_anthropic_api_key",\n)\nopenai_api = AIModelAPI(\n    provider=AIModelAPIProviderEnum.OPEN_AI,\n    api_key="your_openai_api_key",\n)\ngoogle_api = AIModelAPI(\n    provider=AIModelAPIProviderEnum.GOOGLE_AI,\n    api_key="your_google_api_key",\n)\n\n\n# Create various model endpoints\nall_model_endpoints = [\n    AIModelEndpoint(api=anthropic_api, ai_model=Claude37Sonnet),\n    AIModelEndpoint(api=anthropic_api, ai_model=Claude35Haiku),\n    AIModelEndpoint(api=openai_api, ai_model=O3Mini),\n    AIModelEndpoint(api=openai_api, ai_model=GPT4oMini),\n    AIModelEndpoint(api=google_api, ai_model=Gemini20Flash),\n    AIModelEndpoint(api=google_api, ai_model=Gemini20FlashLite),\n]\n\n\ndef get_context(previous_nodes: list[ConversationNode], destination_model: Any) -> list[Any]:\n    """Process previous conversation nodes into context for the next turn."""\n    context = []\n\n    for node in previous_nodes:\n        prompts = PromptFormatter.format_conversion_node_as_prompts(\n            model=destination_model,\n            user_query=node.user_query,\n            attached_files=node.attached_files,\n            previous_response=node.response,\n        )\n        context.extend(prompts)\n\n    return context\n\n\ndef handle_conversation_turn(\n    user_query: str,\n    instructions: list[str],\n    endpoint: AIModelEndpoint,\n    conversation_nodes: list[ConversationNode],\n) -> ConversationNode:\n    """Process a single conversation turn with the specified model and query."""\n\n    client = AIModelClient(\n        model_endpoint=endpoint,\n        config=AIModelCallConfig(\n            max_output_tokens=1000,\n            streaming=False,\n        ),\n        is_async=False,\n    )\n\n    # Format the user query\n    prompt = PromptFormatter.format_conversion_node_as_prompts(\n        model=endpoint.ai_model,\n        user_query=user_query,\n        attached_files=[],\n        previous_response=[],\n    )[0]\n\n    # Get context from previous turns (if any)\n    context = get_context(conversation_nodes, endpoint.ai_model) if conversation_nodes else []\n\n    # Generate response\n    response = client.generate(\n        prompt=prompt,\n        context=context,\n        instructions=instructions,\n    )\n\n    # Create conversation node\n    node = ConversationNode(\n        user_query=user_query,\n        attached_files=[],\n        response=response.chat_response,\n        timestamp=datetime.datetime.now().isoformat(),\n    )\n\n    return node\n\n\ndef run_multi_turn_conversation():\n    multi_turn_queries = [\n        "Tell me a short story about a robot learning to paint.",\n        "Continue the story but add a twist where the robot discovers something unexpected.",\n        "Conclude the story with an inspiring ending.",\n    ]\n\n    # Instructions for each turn\n    instructions_by_turn = [\n        ["Be creative and engaging."],\n        ["Build upon the previous story seamlessly."],\n        ["Bring the story to a satisfying conclusion."],\n    ]\n\n    # Store conversation history\n    conversation_nodes = []\n\n    # Process each turn\n    for i, query in enumerate(multi_turn_queries):\n        # Choose a random model endpoint\n        model_endpoint = random.choice(all_model_endpoints)\n        # OR choose if fixed order as\n        # model_endpoint = all_model_endpoints[i]\n\n        print(f"\ud83d\udd04 Turn {i + 1} with {model_endpoint.ai_model.model_name} from {model_endpoint.api.provider}\\n")\n\n        node = handle_conversation_turn(\n            user_query=query,\n            instructions=instructions_by_turn[i],  # Only if you need to change instruction on each turn, else leave []\n            endpoint=model_endpoint,\n            conversation_nodes=conversation_nodes,\n        )\n\n        # Display the conversation\n        print(f"User: {query}")\n        print(f"Model: {model_endpoint.ai_model.model_name}\\n")\n        print(f"Model Response:\\n {node.response.choices[0].contents[0].get_text()}\\n")\n        print("-" * 80)\n\n        # Append to nodes, so that next turn will have the context generated\n        conversation_nodes.append(node)\n\n\nif __name__ == "__main__":\n    run_multi_turn_conversation()\n\n'})})]})}function u(n={}){const{wrapper:e}={...(0,r.R)(),...n.components};return e?(0,i.jsx)(e,{...n,children:(0,i.jsx)(p,{...n})}):p(n)}},8453:(n,e,o)=>{o.d(e,{R:()=>a,x:()=>s});var t=o(6540);const i={},r=t.createContext(i);function a(n){const e=t.useContext(r);return t.useMemo((function(){return"function"==typeof n?n(e):{...e,...n}}),[e,n])}function s(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(i):n.components||i:a(n.components),t.createElement(r.Provider,{value:e},n.children)}}}]);