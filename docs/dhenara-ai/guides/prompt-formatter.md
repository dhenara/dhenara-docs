---
title: Prompt Formatter
---

# PromptFormatter /Prompt Convertor

One onf the challenge in using models from multiple provider is , they all need prompts in different format. Eg: to
represent previoously generated content, the format could be ` {'role': 'assistant'}` or ` {'role': 'system'}` . Dhenara
makes this convertion easier by using the prompt formatter.

For a specific nodei

```python
from dhenara.ai.providers.common import PromptFormatter

prompts = PromptFormatter.format_conversion_node_as_prompts(
            model=destination_model,
            user_query=self.question,
            attached_files=attached_files,
            previous_response=previous_response,
            max_words_query=max_words_question,
            max_words_files=max_words_files,
            max_words_response=max_words_answer,
            # NOTE: As we reverse the order get_ancestor_prompts, add answer before question
            response_before_query=True,  # NOTE: True. see comments above
        )
```

This will return an array of of prompts that is in the format of the `model`. This is very useful if you want to send a
node ( a user inut and the output generated by a model) as input to a model from a different provider (Say, form open-ai
to anthropic). You will use it like

```python
def main():
    model_endpoint = AIModelEndpoint() # Setup model endpoint correctly

    destination_model = model_endpoint.ai_model

    user_query= "Tell me a joke."

    # Currently there is no Pydantic model defined to represent a Node. We will add this going forward
    previous_nodes.append( {
        "user_query": user_query,
        "attached_files": [],
        "response": response.chat_response,
    })


    prompt = PromptFormatter.format_conversion_node_as_prompts(
            model=destination_model,
            user_query=user_query,
            attached_files=[],
            previous_response=[], # Don't add previous responses here, as it will be passed as the `context`
        )

    context = get_context(previous_nodes)



def get_context( previouse_nodes: dict, destination_model: AIModel,):
    context= []

    for node  in nodes:
        prompts = PromptFormatter.format_conversion_node_as_prompts(
            model=destination_model,
            user_query=node["user_query"],
            attached_files=node["attached_files"],
            previous_response=node["response"],
            # Optionally you cann limit number of workds in question, and pevious response
            max_words_query=None ,
            max_words_files=None,
            max_words_response=None,
            # You can chage the order in which question and anser promts are appended
            response_before_query=True,
        )
        context.extend(prompts)

    return context

```
